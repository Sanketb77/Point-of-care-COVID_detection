{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Binary_Part01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Project Title : - Using Lung ultrasound images for building a reliable Point-of-care Covid-19 testing system"
      ],
      "metadata": {
        "id": "JxiF7Vnsh65q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Binary Classifier : For classifying lung ultrasound images into one of the two categories : Covid and Non Covid"
      ],
      "metadata": {
        "id": "Hh78iiRyiDKk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gOF1utzcIxQG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3890c3b-3529-4295-d8e1-ac920c99bd1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "#Mounting the google drive as the data is available there\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Splitting the dataset into training, testing and validation"
      ],
      "metadata": {
        "id": "ljySKPYMiT54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Training dataset will be used for training the model, validation data will be used for adjusting the hyperparameters. The test dataset will be the unseen set of images, that'd be used for testing model's performance on the unseen dataset. 70% of the entire dataset is used as training, 10% as validation and the rest as testing."
      ],
      "metadata": {
        "id": "F7UDWn8spe2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install split-folders"
      ],
      "metadata": {
        "id": "ght1MEZgiZCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#splitfolders makes it easier for splitting data in form of folders\n",
        "\n",
        "import splitfolders\n",
        "\n",
        "#Splitting the entire dataset into training, testing and validation folders\n",
        "\n",
        "splitfolders.ratio('/content/drive/MyDrive/My_Dataset',output ='binary_two', ratio = (.7,0.1,0.2))"
      ],
      "metadata": {
        "id": "gU36_01ciZOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset Verification"
      ],
      "metadata": {
        "id": "o2mdfC-0iiFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### The following piece of code checks the number of images in each class for all the three types of dataset."
      ],
      "metadata": {
        "id": "N9TjwXbxqGWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Training dataset\")\n",
        "files = os.listdir(\"/content/drive/MyDrive/binary_two/train/covid\")\n",
        "file_count = len(files)\n",
        "print(\"Class Covid : \",file_count)\n",
        "\n",
        "files = os.listdir(\"/content/drive/MyDrive/binary_two/train/non_covid\")\n",
        "file_count = len(files)\n",
        "print(\"Class Non Covid: \",file_count)\n",
        "\n",
        "\n",
        "\n",
        "print(\"---------------------\")\n",
        "print(\"Testing dataset\")\n",
        "files = os.listdir(\"/content/drive/MyDrive/binary_two/test/covid\")\n",
        "file_count = len(files)\n",
        "print(\"Class Covid : \",file_count)\n",
        "\n",
        "files = os.listdir(\"/content/drive/MyDrive/binary_two/test/non_covid\")\n",
        "file_count = len(files)\n",
        "print(\"Class Non Covid: \",file_count)\n",
        "\n",
        "\n",
        "print(\"---------------------\")\n",
        "print(\"Validation dataset\")\n",
        "files = os.listdir(\"/content/drive/MyDrive/binary_two/val/covid\")\n",
        "file_count = len(files)\n",
        "print(\"Class Covid : \",file_count)\n",
        "\n",
        "files = os.listdir(\"/content/drive/MyDrive/binary_two/val/non_covid\")\n",
        "file_count = len(files)\n",
        "print(\"Class Non Covid: \",file_count)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjxBVZZecwAA",
        "outputId": "05fc6869-0f0c-4d44-9cc1-89c24cc24ecc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset\n",
            "Class Covid :  2831\n",
            "Class Non Covid:  6860\n",
            "---------------------\n",
            "Testing dataset\n",
            "Class Covid :  810\n",
            "Class Non Covid:  1961\n",
            "---------------------\n",
            "Validation dataset\n",
            "Class Covid :  404\n",
            "Class Non Covid:  980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset Loading and Pre-processing"
      ],
      "metadata": {
        "id": "7LRK1Qf9izCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### It is important to pre-process the images before they are fed into the model. The image pre-processing has been done using Image Data Generator, as the dataset is huge and it'd be feasible to input data in batches. the batch size chosen is 512. The images are rescaled, so that the pixel values lie between 0-1 for all the images. The images have been resized to 197 X 198 pixels."
      ],
      "metadata": {
        "id": "xUdCyPI8qQwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the necessary libraries\n",
        "\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "#Creating path for dataset folders\n",
        "p = Path().cwd()\n",
        "q = p/'binary_two'\n",
        "\n",
        "#Images would be fed into the model as a batch of this size\n",
        "batch_size = 512\n",
        "\n",
        "#Rescaling the pixel values\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "#Creating data generators for all the three datasets\n",
        "train_generator = train_datagen.flow_from_directory(q/'train',target_size=(197,198), batch_size= batch_size,class_mode ='categorical',shuffle = False)\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(q/'test',target_size= (197,198), batch_size= batch_size,class_mode ='categorical',shuffle = False)\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(q/'val',target_size=(197,198), batch_size= batch_size,class_mode ='categorical',shuffle = False)\n",
        "\n",
        "        # confirm the iterator works\n",
        "\n",
        "batchX, batchy = train_generator.next()\n",
        "print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))\n"
      ],
      "metadata": {
        "id": "om9HkQ9SoFLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5954a860-a326-4f6f-902a-df378dc0ebf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9691 images belonging to 2 classes.\n",
            "Found 2771 images belonging to 2 classes.\n",
            "Found 1384 images belonging to 2 classes.\n",
            "Batch shape=(512, 197, 198, 3), min=0.000, max=1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating and Training the baseline model**"
      ],
      "metadata": {
        "id": "ZwO6HlS-i3bH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### VGG-16 has been chosen as the baseline model. Given the large amount of data, it's easier to use a pre-trained network for the classification. "
      ],
      "metadata": {
        "id": "75mlC0uorPW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# include_top = 'False' as we only want the feature extractor part and not the classifier part. The input shape has been given as per our image dataset.\n",
        "\n",
        "pretrained= tf.keras.applications.VGG16(include_top=False,\n",
        "                   input_shape=(197,198,3), weights='imagenet')\n",
        "\n",
        "#We want the model to retain the parameters (weights and biases) it has learned from the image dataset, hence we are freezing the feature extractor layers.\n",
        "pretrained.trainable=False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzObxQyFold3",
        "outputId": "21de5840-3f27-40a8-868f-0a94b6967913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Functional API has been used for adding classifier layers to the pre-trained part.\n",
        "\n",
        "inputs = tf.keras.Input(shape=(197,198,3))\n",
        "x = pretrained(inputs)\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "x = tf.keras.layers.Dense(1024)(x)\n",
        "x = tf.keras.layers.Dense(512)(x)\n",
        "outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "multi_vgg16 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "multi_vgg16.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mycyvduoog7",
        "outputId": "96a985aa-4008-4e1f-dcac-4e5051132eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 197, 198, 3)]     0         \n",
            "                                                                 \n",
            " vgg16 (Functional)          (None, 6, 6, 512)         14714688  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 18432)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              18875392  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               524800    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 34,115,906\n",
            "Trainable params: 19,401,218\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The above model has been compiled before using it.\n",
        "\n",
        "multi_vgg16.compile(loss=\"categorical_crossentropy\",optimizer= 'rmsprop' , metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "h87Y9mUmozGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The model has been fit to the train data\n",
        "\n",
        "baseline_model = multi_vgg16.fit( train_generator,steps_per_epoch = len(train_generator),epochs= 5 , validation_data= val_generator,\n",
        "             validation_steps = len(val_generator))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhLVfBk6oz8n",
        "outputId": "bcf7ef10-7ac0-4d37-fb22-34a9c00c544a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "19/19 [==============================] - 4965s 258s/step - loss: 123.7740 - accuracy: 0.5699 - val_loss: 4.8110 - val_accuracy: 0.5101\n",
            "Epoch 2/5\n",
            "19/19 [==============================] - 80s 4s/step - loss: 14.5476 - accuracy: 0.5193 - val_loss: 7.1834 - val_accuracy: 0.3309\n",
            "Epoch 3/5\n",
            "19/19 [==============================] - 80s 4s/step - loss: 3.4794 - accuracy: 0.8221 - val_loss: 0.5935 - val_accuracy: 0.8150\n",
            "Epoch 4/5\n",
            "19/19 [==============================] - 80s 4s/step - loss: 32.2937 - accuracy: 0.6502 - val_loss: 1.4682 - val_accuracy: 0.8822\n",
            "Epoch 5/5\n",
            "19/19 [==============================] - 80s 4s/step - loss: 0.9334 - accuracy: 0.8969 - val_loss: 0.2555 - val_accuracy: 0.9342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Checking model's classification performance on the test data"
      ],
      "metadata": {
        "id": "cd3U4KPSjATm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### The model's actual performance that matters, is the one it gives on the unseen dataset. The following code gets prediction class for each image in the test dataset and compare them with the true class of the image for getting a true picture of model's performance."
      ],
      "metadata": {
        "id": "GqX7teHasRvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "#Making predictions on the test data\n",
        "\n",
        "predict = multi_vgg16.predict(\n",
        "    test_generator, steps=None, callbacks=None, max_queue_size=10, workers=1,\n",
        "    use_multiprocessing=False, verbose=0\n",
        ")\n",
        "\n",
        "#Getting actual prediction class from the predictions made\n",
        "\n",
        "vgg_pred_classes_ft = np.argmax(predict, axis=1)\n",
        "\n",
        "#The true classes\n",
        "true_classes = test_generator.classes\n",
        "\n",
        "#Calculating performance metrics\n",
        "\n",
        "print(\"Precision score is \", average_precision_score(true_classes, vgg_pred_classes_ft))\n",
        "print(\"Accuracy score is \", accuracy_score(true_classes, vgg_pred_classes_ft))\n",
        "print(\"Recall is \",recall_score(true_classes, vgg_pred_classes_ft))\n",
        "print(\"F1 score is \",f1_score(true_classes, vgg_pred_classes_ft))"
      ],
      "metadata": {
        "id": "GGQQsCeJo6_e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bfac028-d687-4279-eb1d-1551979fbf76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision score is  0.6673778889168532\n",
            "Accuracy score is  0.5520491803278689\n",
            "Recall is  0.6719512195121952\n",
            "F1 score is  0.6684865028814073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Confusion matrix - The confusion matrix makes it easier to look at the predictions made by the model. The following code plots the same for the test dataset."
      ],
      "metadata": {
        "id": "ruU5aLfkjFyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Get the names of the ten classes\n",
        "class_names = test_generator.class_indices.keys()\n",
        "\n",
        "true_classes = test_generator.classes\n",
        "\n",
        "class_names = test_generator.class_indices.keys()\n",
        "\n",
        "def plot_heatmap(y_true, y_pred, class_names, ax, title):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    sns.heatmap(\n",
        "        cm, \n",
        "        annot=True, \n",
        "        square=True, \n",
        "        xticklabels=class_names, \n",
        "        yticklabels=class_names,\n",
        "        fmt='d', \n",
        "        cmap=plt.cm.Blues,\n",
        "        cbar=False,\n",
        "        ax=ax\n",
        "    )\n",
        "    ax.set_title(title, fontsize=16)\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
        "    ax.set_ylabel('True Label', fontsize=12)\n",
        "    ax.set_xlabel('Predicted Label', fontsize=12)\n",
        "\n",
        "fig, (ax1) = plt.subplots(1, 1, figsize=(20, 10))\n",
        "\n",
        "plot_heatmap(true_classes, vgg_pred_classes_ft, class_names, ax1, title=\"Baseline model\")    \n",
        "\n",
        "fig.suptitle(\"Confusion Matrix\", fontsize=24)\n",
        "fig.tight_layout()\n",
        "fig.subplots_adjust(top=1.25)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8JXp_r39pEJZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        },
        "outputId": "146d4d76-f288-44a7-d3f7-d9d60701f146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Evidently, the model is better at identifying lung ultrasound images of non covid patients than the covid positive patients. A part of the reason can be, the covid images are limited and the model is getting to learn more from the other category.We'd now try various techniques to deal with the same and performance will be compared to this baseline. "
      ],
      "metadata": {
        "id": "WFLzMSJftJ5g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Offline Data Augmentation on COVID class**"
      ],
      "metadata": {
        "id": "6Nqt-Y1HudMC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### The lung ultrasound images of COVID positive cases in training dataset are roughly around 2700, whereas those for COVID negative are around 8000. That is a massive class imbalance. The following chunk of code creates more images in COVID class from the existing images by transforming them."
      ],
      "metadata": {
        "id": "onR_OhLrtxIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from skimage import io\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "\n",
        "# This transformation will be performed on training images at random, this includes rotating the images by 40 degrees, flipping them horizontally.\n",
        "\n",
        "#Changing the brightness level and zooming in\n",
        "\n",
        "datagen = ImageDataGenerator(        \n",
        "        rotation_range = 40,\n",
        "        shear_range = 0.2,\n",
        "        zoom_range = 0.2,\n",
        "        horizontal_flip = True,\n",
        "        brightness_range = (0.5, 1.5))\n",
        "\n",
        "p = Path().cwd()\n",
        "image_directory =  p/'binary_split/train/covid'\n",
        "\n",
        "\n",
        "dataset = []\n",
        "my_images = os.listdir(image_directory)\n",
        "\n",
        "for i, image_name in enumerate(my_images): \n",
        "\n",
        "    if (image_name.split('.')[1] == 'jpg'): \n",
        "        path = os.path.join(image_directory,image_name)       \n",
        "        image = io.imread(path)        \n",
        "        image = Image.fromarray(image, 'RGB')        \n",
        "        image = image.resize((224,224)) \n",
        "        dataset.append(np.array(image))\n",
        "\n",
        "# A new folder has been created for saving these newly created images       \n",
        "x = np.array(dataset)\n",
        "i = 0\n",
        "for batch in datagen.flow(x, batch_size=16,\n",
        "                          save_to_dir= \"/content/drive/MyDrive/augmented\",\n",
        "                          save_prefix='dr',\n",
        "                          save_format='jpg'):    \n",
        "    i += 1    \n",
        "    if i > 300:        \n",
        "        break"
      ],
      "metadata": {
        "id": "8xOBieKZunCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This offline augmentation has created around 5000 images of covid class\n",
        "\n",
        "files = os.listdir(\"/content/drive/MyDrive/augmented\")\n",
        "file_count = len(files)\n",
        "print(file_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrnU98ipZo8r",
        "outputId": "77b0a153-078f-48b4-a01f-a3505691ffd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The same have been moved to the training dataset\n",
        "\n",
        "path = \"/content/drive/MyDrive/augmented\"\n",
        "d_path = \"/content/drive/MyDrive/binary_split/train/covid\"\n",
        "\n",
        "for file in os.listdir(path):\n",
        "        s = os.path.join(path,file)\n",
        "        new_path = shutil.move(s,d_path)"
      ],
      "metadata": {
        "id": "mthJRHv9h3-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The training dataset now has 7608 lung ultrasound images of class - COVID\n",
        "\n",
        "files = os.listdir(\"/content/drive/MyDrive/binary_split/train/covid\")\n",
        "file_count = len(files)\n",
        "print(file_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZYKeVyBj6De",
        "outputId": "85eb95ec-e99b-4d15-ff97-8c6ba25708d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset Loading and Pre-processing"
      ],
      "metadata": {
        "id": "SV70fvunj0vl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Since we are inputting data using image data generator and pre-processing the images while feeding them to the model. This step has to be repeated again for the next model."
      ],
      "metadata": {
        "id": "gtR0Y-DHvWOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the necessary libraries\n",
        "\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "  #Creating path for dataset folders\n",
        "p = Path().cwd()\n",
        "q = p/'binary_split'\n",
        "\n",
        "  #Images would be fed into the model as a batch of this size\n",
        "batch_size = 512\n",
        "\n",
        "        #Rescaling the pixel values\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "        #Creating data generators for all the three datasets\n",
        "train_generator = train_datagen.flow_from_directory(q/'train',target_size=(224,224), batch_size= batch_size,class_mode ='categorical')\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(q/'test',target_size= (224,224), batch_size= batch_size,class_mode ='categorical', shuffle = False)\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(q/'val',target_size=(224,224), batch_size= batch_size,class_mode ='categorical')\n",
        "\n",
        "        # confirm the iterator works\n",
        "\n",
        "batchX, batchy = train_generator.next()\n",
        "print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW5SR9iKkylM",
        "outputId": "0440f3ca-4996-42cd-dc2f-ca2114039037"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 15870 images belonging to 2 classes.\n",
            "Found 2440 images belonging to 2 classes.\n",
            "Found 1579 images belonging to 2 classes.\n",
            "Batch shape=(512, 224, 224, 3), min=0.000, max=1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### VGG-19"
      ],
      "metadata": {
        "id": "uf-80dsp_rUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained= tf.keras.applications.VGG19(include_top=False,\n",
        "                   input_shape=(224,224,3), weights='imagenet')\n",
        "\n",
        "pretrained.trainable=False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzVNSh7h_tJf",
        "outputId": "ffb49383-02f2-4fc6-d6f6-31366c005460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 0s 0us/step\n",
            "80150528/80134624 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tf.keras.Input(shape=(224,224,3))\n",
        "x = pretrained(inputs)\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "x = tf.keras.layers.Dense(1024)(x)\n",
        "x = tf.keras.layers.Dense(512)(x)\n",
        "outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "vgg19 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "#Getting the model architecture\n",
        "vgg19.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyZknuKy_8rw",
        "outputId": "359228d6-a152-4e1c-899b-459645177b20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " vgg19 (Functional)          (None, 7, 7, 512)         20024384  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1024)              25691136  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               524800    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 46,241,346\n",
            "Trainable params: 26,216,962\n",
            "Non-trainable params: 20,024,384\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compiling the model\n",
        "\n",
        "vgg19.compile(loss=\"categorical_crossentropy\",optimizer = 'rmsprop', metrics=[\"accuracy\"])\n",
        "\n",
        "#We wish to monitor the validation loss and save the best model\n",
        "\n",
        "callbacks = tf.keras.callbacks.ModelCheckpoint(filepath=\"fine_tuning.keras\",\n",
        "                                            save_best_only=True,\n",
        "                                            monitor=\"val_loss\")\n"
      ],
      "metadata": {
        "id": "dknN3bmG__ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the model\n",
        "\n",
        "network_1 = vgg19.fit( train_generator,steps_per_epoch = len(train_generator),epochs= 5 , validation_data= val_generator,\n",
        "             validation_steps = len(val_generator), callbacks=[callbacks])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hV-AOKXADcB",
        "outputId": "b43a26fe-0d04-4236-e34b-8be2ec2b1de0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "31/31 [==============================] - 5835s 188s/step - loss: 42.8404 - accuracy: 0.7369 - val_loss: 0.2109 - val_accuracy: 0.9272\n",
            "Epoch 2/5\n",
            "31/31 [==============================] - 107s 3s/step - loss: 17.4931 - accuracy: 0.8537 - val_loss: 0.3577 - val_accuracy: 0.9411\n",
            "Epoch 3/5\n",
            "31/31 [==============================] - 109s 3s/step - loss: 0.5628 - accuracy: 0.9425 - val_loss: 0.0125 - val_accuracy: 0.9975\n",
            "Epoch 4/5\n",
            "31/31 [==============================] - 106s 3s/step - loss: 11.0937 - accuracy: 0.8342 - val_loss: 0.0231 - val_accuracy: 0.9905\n",
            "Epoch 5/5\n",
            "31/31 [==============================] - 106s 3s/step - loss: 0.0410 - accuracy: 0.9894 - val_loss: 6.1886 - val_accuracy: 0.8309\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model's performance on test dataset"
      ],
      "metadata": {
        "id": "K_5KDXZdD8Jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "#Loading the best model saved\n",
        "test_model = tf.keras.models.load_model(\"fine_tuning.keras\")\n",
        "\n",
        "#Using the same for making predictions\n",
        "predict = test_model.predict_generator(\n",
        "    test_generator, workers=0, verbose=0\n",
        ")\n",
        "\n",
        "predictedClass = np.argmax(predict, axis=1)\n",
        "\n",
        "#The actual classes\n",
        "trueClass = test_generator.classes[test_generator.index_array]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_UIv7BpiOtf",
        "outputId": "6bc8becf-f533-4bfc-afcf-581a8d6b8694"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix,plot_confusion_matrix\n",
        "\n",
        "#Confusion matrix for the predictions made\n",
        "\n",
        "confusionMatrix = (confusion_matrix(\n",
        "        y_true = trueClass,                                       \n",
        "        y_pred = predictedClass))                                \n",
        "\n",
        "print(confusionMatrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK82zf703jZ7",
        "outputId": "87c659c8-0f62-4365-a4c6-f2590905d3d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 294  506]\n",
            " [  19 1621]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The performance scores of the model\n",
        "\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "print(\"Precision score is \", precision_score(trueClass, predictedClass))\n",
        "print(\"Accuracy score is \", accuracy_score(trueClass, predictedClass))\n",
        "print(\"Recall is \",recall_score(trueClass, predictedClass))\n",
        "print(\"F1 score is \",f1_score(trueClass, predictedClass))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZYatrl_yx-c",
        "outputId": "ebb4d28e-2c72-4eb3-89b4-c0ca6b67fe47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision score is  0.7621062529384109\n",
            "Accuracy score is  0.7848360655737705\n",
            "Recall is  0.9884146341463415\n",
            "F1 score is  0.8606318024953544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The prediction accuracy improved to 78.4% with data augmentation on COVID class."
      ],
      "metadata": {
        "id": "-aNfJlMJEHQQ"
      }
    }
  ]
}